---
title: "DATA: STORAGE, TRADEOFFS, AND SCALING BASICS"
date: "2025-01-29"
summary: "Exploring concepts related to data storage, tradeoffs, and scaling of such databases."
author: "Advait Deshmukh"
readTime: "6 mins"
---

Data storage and scaling are fundamental challenges in modern computing. This post explores key concepts and strategies for managing data at scale, from data center architecture to database scaling approaches.

### Data Center Architecture

Data Centers are all about housing our Computer Systems within a building, or a group of buildings. Making effective choices with regards to choosing our servers is necessary for a cost-effective and scalable infrastructure design.

## Low-End vs. High-End servers

### Cost Efficiency

A low end server will be less powerful than a high end server. However, it does not make sense to pick multiple high-end servers for our Data Center.
Using multiple low end servers, is 5% slower on average, than a few high end servers.
High-end servers come with a steep price premium, a disproportionate cost for marginal performance gains.

### Scalability

A few high-end servers would suffer from performance degradation, as the cluster size grows. This is due to coordination overhead, network latency, or resource contention.

Instead, having multiple high end servers is better for scaling horizontally -- we add more nodes, much more affordably, avoiding bottlenecks tied to centralized high-end hardware.

## Storage Architecture

Data centers typically employ a distributed approach to storage where disks are connected directly to individual servers using a global distributed file system. This architecture offers several advantages:

- High availability even when individual servers fail
- Flexible write overhead tradeoffs
- Cost-effective scaling
- Enhanced read bandwidth through data locality
- Ability to process data locally where it's stored

### Networking Infrastructure
The network architecture typically consists of:

- Commodity ethernet switches (1 Gbps) with up to 48 ports
- Racks containing up to 40 servers each
- 4-8 4 Gbps uplinks per rack
- Dedicated switches for inter-rack communication
- Specialized cluster-level switches for backbone connectivity

## Database Fundamentals

Databases serve as the cornerstone of data storage, aiming to achieve three primary objectives:
1. Fast read operations
2. Fast write operations
3. Data persistence and durability

### Index Implementations
Indexes are crucial for optimizing read performance, though they come with storage and write overhead costs. Three main approaches exist:

1. Hash Indexes
   - Implement a hash map structure
   - Keys map directly to memory locations
   - Limited by available memory
   - Poor performance for range queries

2. SSTables and LSM Trees
   - Utilize a memory buffer (memtable) implemented as a self-balancing tree
   - Automatically flush to SSTable files when memory limits are reached

## Database Scaling Strategies

Database scaling often becomes the primary performance bottleneck in web applications. With typical web applications seeing 95%+ read operations, scaling strategies must be carefully considered.

### Basic Optimization Techniques

1. Strategic Indexing
   - Create indexes on frequently accessed columns
   - Improves read performance
   - Trades off write speed and storage space

2. Denormalization
   - Adds redundant data to reduce joins
   - Significantly improves read performance
   - Increases complexity of write operations
   - Makes data consistency more challenging

3. Connection Pooling
   - Enables multiple application threads to share database connections
   - Reduces connection overhead

4. Caching
   - Implements intermediate storage layer (e.g., Redis, Memcached)
   - Not suitable for highly dynamic data
   - Reduces database load for frequently accessed data

### Advanced Scaling Techniques

#### Replication
- Implements read replicas for distributing read load
- Dedicates master server for write operations
- Provides built-in fault tolerance
- Requires careful management of data consistency

#### Partitioning
Two main approaches exist:

1. Sharding (Horizontal Partitioning)
   - Splits data across multiple databases while maintaining schema
   - Improves traffic handling
   - Challenges include hot keys and cross-shard joins

2. Vertical Partitioning
   - Divides database schema by functionality
   - Optimal when queries typically need limited data subsets

## The NoSQL Alternative

With data generation exceeding 30,000 GB per second and growing, traditional RDBMS solutions often struggle to scale effectively. NoSQL databases offer an alternative approach:

- Simplified querying through key-lookup systems
- Better performance predictability
- Scales to massive datasets
- Trades off some ACID guarantees for performance
- Distributes load across multiple servers
- Implements sharding through consistent hash rings or range partitioning

### When to Consider NoSQL

NoSQL becomes particularly attractive when:
- Perfect consistency isn't always required
- Dataset size exceeds traditional RDBMS capabilities
- Query patterns align with key-value lookups
- Horizontal scaling is a priority

## Migration Strategies

When transitioning between systems, several approaches can be employed:

- Dark Reads: Testing new system reads without affecting production
- Dark Writes: Parallel writing to verify new system capability
- Light Reads: Gradual integration of new system reads
- Light Writes: Controlled introduction of new system writes

The choice between maintaining a traditional RDBMS and migrating to NoSQL often depends on your specific use case, scale requirements, and consistency needs. Early consideration of these factors can significantly impact the long-term success of your data architecture.